{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ernest\\.conda\\envs\\l2rpn-test\\lib\\site-packages\\grid2op\\Backend\\pandaPowerBackend.py:32: UserWarning: Numba cannot be loaded. You will gain possibly massive speed if installing it by \n",
      "\tc:\\Users\\Ernest\\.conda\\envs\\l2rpn-test\\python.exe -m pip install numba\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import yaml\n",
    "import random\n",
    "from utils.converter import Converter\n",
    "from utils.replay_buffer import ReplayBuffer\n",
    "from agents.network import QNetwork\n",
    "from utils.data_saver import TrajectoryDataLoader\n",
    "import grid2op \n",
    "from grid2op.Action import TopologyChangeAction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OfflineQAgent:\n",
    "    def __init__(self, cfg, env):\n",
    "        self.cfg = cfg\n",
    "        self.env = env\n",
    "        self.converter = Converter(self.env)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.memory = ReplayBuffer(self.cfg['MEM_SIZE'], self.cfg['BATCH_SIZE']) \n",
    "        self.q_net = QNetwork(self.cfg).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=self.cfg['LR'])\n",
    "        self.losses = []\n",
    "        self.exploration_rate = self.cfg['EXPLORATION_MAX']\n",
    "\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.random() > self.cfg['EPSILON_RATE']:\n",
    "            return self.converter.convert_env_act_to_one_hot_encoding_act(self.env.action_space.sample().to_vect())\n",
    "        \n",
    "        \n",
    "        state = torch.tensor(state).to(self.device)\n",
    "        q_val = self.q_net(state)\n",
    "        return torch.argmax(q_val).item()\n",
    "    \n",
    "\n",
    "    def learn(self, states, actions, rewards, states_, dones):\n",
    "        #states = torch.tensor(states , dtype=torch.float32).to(self.device)\n",
    "        #actions = torch.tensor(actions, dtype=torch.long).to(self.device)\n",
    "        #rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n",
    "        #states_ = torch.tensor(states_, dtype=torch.float32).to(self.device)\n",
    "        #dones = torch.tensor(dones, dtype=torch.bool).to(self.device)\n",
    "        states = states.to(self.device)\n",
    "        actions = actions.to(self.device)\n",
    "        rewards = rewards.to(self.device)\n",
    "        states_ = states_.to(self.device)\n",
    "        dones = dones.to(self.device)\n",
    "        batch_indices = np.arange(self.cfg['BATCH_SIZE'], dtype=np.int64)\n",
    "\n",
    "        q_values = self.q_net(states)\n",
    "        next_q_values = self.q_net(states_)\n",
    "        \n",
    "        predicted_value_of_now = q_values[batch_indices, actions]\n",
    "        predicted_value_of_future = torch.max(next_q_values, dim=1)[0]\n",
    "        \n",
    "        q_target = rewards + self.cfg['GAMMA'] * predicted_value_of_future * dones\n",
    "\n",
    "        loss = self.q_net.loss(q_target, predicted_value_of_now)\n",
    "        self.losses.append(loss)\n",
    "        self.q_net.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.q_net.optimizer.step()\n",
    "\n",
    "        self.exploration_rate *= self.cfg['EXPLORATION_DECAY']\n",
    "        self.exploration_rate = max(self.cfg['EXPLORATION_MIN'], self.exploration_rate)\n",
    "\n",
    "\n",
    "    def returning_epsilon(self):\n",
    "        return self.exploration_rate\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        torch.save(self.q_net.state_dict(), os.path.join(path, \"offlineDQN.pth\"))\n",
    "\n",
    "    def load_model(self, path):\n",
    "        torch.load(self.q_net.load_state_dict(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_yaml_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        try:\n",
    "            data = yaml.safe_load(file)\n",
    "            return data\n",
    "        except yaml.YAMLError as e:\n",
    "            print(f\"Error reading YAML file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ernest\\.conda\\envs\\l2rpn-test\\lib\\site-packages\\grid2op\\MakeEnv\\Make.py:438: UserWarning: You are using a development environment. This environment is not intended for training agents. It might not be up to date and its primary use if for tests (hence the \"test=True\" you passed as argument). Use at your own risk.\n",
      "  warnings.warn(_MAKE_DEV_ENV_WARN)\n"
     ]
    }
   ],
   "source": [
    "env_name = \"rte_case5_example\"  # or any other name.\n",
    "env = grid2op.make(env_name, test=True, action_class=TopologyChangeAction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TrajectoryDataLoader(\"Data\\\\trajectory.pkl\", batch_size=32)\n",
    "yaml_data = read_yaml_file(\"config.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = OfflineQAgent(env=env, cfg=yaml_data)\n",
    "converter = Converter(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "Episode 2\n",
      "Episode 3\n",
      "Episode 4\n",
      "Episode 5\n",
      "Episode 6\n",
      "Episode 7\n",
      "Episode 8\n",
      "Episode 9\n",
      "Episode 10\n",
      "Episode 11\n",
      "Episode 12\n",
      "Episode 13\n",
      "Episode 14\n",
      "Episode 15\n",
      "Episode 16\n",
      "Episode 17\n",
      "Episode 18\n",
      "Episode 19\n",
      "Episode 20\n",
      "Episode 21\n",
      "Episode 22\n",
      "Episode 23\n",
      "Episode 24\n",
      "Episode 25\n",
      "Episode 26\n",
      "Episode 27\n",
      "Episode 28\n",
      "Episode 29\n",
      "Episode 30\n",
      "Episode 31\n",
      "Episode 32\n",
      "Episode 33\n",
      "Episode 34\n",
      "Episode 35\n",
      "Episode 36\n",
      "Episode 37\n",
      "Episode 38\n",
      "Episode 39\n",
      "Episode 40\n",
      "Episode 41\n",
      "Episode 42\n",
      "Episode 43\n",
      "Episode 44\n",
      "Episode 45\n",
      "Episode 46\n",
      "Episode 47\n",
      "Episode 48\n",
      "Episode 49\n"
     ]
    }
   ],
   "source": [
    "#score = 0\n",
    "best_score = 0\n",
    "agent = OfflineQAgent(env=env, cfg=yaml_data)\n",
    "\n",
    "for i in range(1, 50):\n",
    "    print(f\"Episode {i}\")\n",
    "    for batch in data:\n",
    "        states, actions, rewards, next_states, dones = batch\n",
    "        if states.shape != torch.Size([16, 182]):\n",
    "            agent.learn(states, actions, rewards, next_states, dones)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 Score 11673.204224586487\n",
      "Episode 1 Score 11101.55497789383\n",
      "Episode 2 Score 13711.604459285736\n",
      "Episode 3 Score 13078.260832071304\n",
      "Episode 4 Score 13340.005165815353\n",
      "Episode 5 Score 1758.710616350174\n",
      "Episode 6 Score 14088.209115505219\n",
      "Episode 7 Score 13795.545959234238\n",
      "Episode 8 Score 12642.432779788971\n",
      "Episode 9 Score 13703.679738044739\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    score = 0\n",
    "    obs = env.reset()\n",
    "    while True:\n",
    "        action = agent.choose_action(obs.to_vect())\n",
    "        obs, reward, done, _ = env.step(converter.convert_one_hot_encoding_act_to_env_act(converter.int_one_hot(action)))\n",
    "        #print(reward)\n",
    "        score+= reward\n",
    "\n",
    "        if done:\n",
    "            print(f\"Episode {i} Score {score}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Very Offline RL Simulation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a1fba7b9e6c44de81f2ee55e7fced8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2016 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "all_obs = []\n",
    "obs = env.reset()\n",
    "all_obs.append(obs)\n",
    "reward = env.reward_range[0]\n",
    "reward_list = []\n",
    "done = False\n",
    "nb_step = 0\n",
    "print(\"Very Offline RL Simulation\")\n",
    "\n",
    "\n",
    "with tqdm(total=env.chronics_handler.max_timestep()) as pbar:\n",
    "    while True:\n",
    "        action = agent.choose_action(obs.to_vect())\n",
    "        #action = my_agent.act(obs, reward, done)\n",
    "        obs, reward, done, _ = env.step(converter.convert_one_hot_encoding_act_to_env_act(converter.int_one_hot(action)))\n",
    "        reward_list.append(reward)\n",
    "        pbar.update(1)\n",
    "        if done:\n",
    "            break\n",
    "        all_obs.append(obs)\n",
    "        nb_step += 1\n",
    "\n",
    "reward_list_simple_DQN = np.copy(reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_model(\"./Agents/OfflineDQN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "l2rpn-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
