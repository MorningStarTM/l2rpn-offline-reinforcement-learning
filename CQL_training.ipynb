{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import yaml\n",
    "import pickle\n",
    "from utils.replay_buffer import ReplayBuffer\n",
    "from agents.network import QNetwork\n",
    "from utils.converter import Converter\n",
    "import grid2op\n",
    "from grid2op.Action import TopologyChangeAction\n",
    "from utils.data_saver import TrajectoryDataLoader\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CQLAgent:\n",
    "    def __init__(self, cfg):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.cfg = cfg\n",
    "        self.q_net = QNetwork(self.cfg).to(self.device)\n",
    "        self.target_net = QNetwork(self.cfg).to(self.device)\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=self.cfg['lr'])\n",
    "        self.tau = 1e-3\n",
    "        \n",
    "    def update_target_network(self):\n",
    "        for target_param, param in zip(self.target_net.parameters(), self.q_net.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "    \n",
    "    def choose_action(self, observation):\n",
    "        state = torch.tensor(observation).to(self.device)\n",
    "        q_val = self.q_net(state)\n",
    "        return torch.argmax(q_val).item()\n",
    "\n",
    "    def learn(self, states, actions, rewards, states_, dones):\n",
    "\n",
    "        states = states.to(self.device)\n",
    "        actions = actions.to(self.device)\n",
    "        rewards = rewards.to(self.device)\n",
    "        states_ = states_.to(self.device)\n",
    "        dones = dones.to(self.device)\n",
    "        batch_indices = np.arange(self.cfg['BATCH_SIZE'], dtype=np.int64)\n",
    "\n",
    "        q_values = self.q_net(states)\n",
    "        next_q_values = self.target_net(states_)\n",
    "\n",
    "        #cql loss\n",
    "        logsump = torch.logsumexp(q_values, keepdim=True, dim=1)\n",
    "        cql_loss = torch.mean(logsump - q_values)\n",
    "        \n",
    "\n",
    "        q_loss = nn.functional.mse_loss(q_values, next_q_values)\n",
    "        #print(q_loss , cql_loss.item() , self.cfg['cql_alpha'])\n",
    "        \n",
    "        total_loss = q_loss + cql_loss.item() * 0.5\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.update_target_network()\n",
    "\n",
    "        return total_loss.item()\n",
    "    \n",
    "\n",
    "    def save_models(self, path):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        torch.save(self.q_net.state_dict(), os.path.join(path, \"CQL.pth\"))\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        self.q_net.load_state_dict(torch.save(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_yaml_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        try:\n",
    "            data = yaml.safe_load(file)\n",
    "            return data\n",
    "        except yaml.YAMLError as e:\n",
    "            print(f\"Error reading YAML file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ernest\\.conda\\envs\\l2rpn-test\\lib\\site-packages\\grid2op\\MakeEnv\\Make.py:438: UserWarning: You are using a development environment. This environment is not intended for training agents. It might not be up to date and its primary use if for tests (hence the \"test=True\" you passed as argument). Use at your own risk.\n",
      "  warnings.warn(_MAKE_DEV_ENV_WARN)\n"
     ]
    }
   ],
   "source": [
    "yaml_data = read_yaml_file('config.yml')\n",
    "data = TrajectoryDataLoader(\"Data\\\\trajectory.pkl\", batch_size=32)\n",
    "\n",
    "env_name = \"rte_case5_example\"  # or any other name.\n",
    "env = grid2op.make(env_name, test=True, action_class=TopologyChangeAction)\n",
    "converter = Converter(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29243b2e4f50420aaa4dfce7a9b3a5eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episodes:   0%|          | 0/499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "agent = CQLAgent(yaml_data)\n",
    "\n",
    "for i in tqdm(range(1, 500), desc=\"Episodes\"):\n",
    "    for batch in data:\n",
    "        states, actions, rewards, next_states, dones = batch\n",
    "        if states.shape != torch.Size([16, 182]):\n",
    "            agent.learn(states, actions, rewards, next_states, dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Very CQL Simulation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20cf7e4b3f984f6989c1739d69dc931a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2016 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "all_obs = []\n",
    "obs = env.reset()\n",
    "all_obs.append(obs)\n",
    "reward = env.reward_range[0]\n",
    "reward_list = []\n",
    "done = False\n",
    "nb_step = 0\n",
    "print(\"Very CQL Simulation\")\n",
    "\n",
    "\n",
    "with tqdm(total=env.chronics_handler.max_timestep()) as pbar:\n",
    "    while True:\n",
    "        action = agent.choose_action(obs.to_vect())\n",
    "        #action = my_agent.act(obs, reward, done)\n",
    "        obs, reward, done, _ = env.step(converter.convert_one_hot_encoding_act_to_env_act(converter.int_one_hot(action)))\n",
    "        reward_list.append(reward)\n",
    "        pbar.update(1)\n",
    "        if done:\n",
    "            break\n",
    "        all_obs.append(obs)\n",
    "        nb_step += 1\n",
    "\n",
    "reward_list_simple_DQN = np.copy(reward_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_models(\"Agents/CQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "l2rpn-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
